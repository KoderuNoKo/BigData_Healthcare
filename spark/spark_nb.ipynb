{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50343da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/dev/app/.venv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c870f1ad-fd3b-4d47-a2c6-c0abcea66ea7;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 793ms :: artifacts dl 26ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c870f1ad-fd3b-4d47-a2c6-c0abcea66ea7\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/19ms)\n",
      "25/12/04 15:34:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Kafka Topics:\n",
      "--------------------------------------------------\n",
      "Topic: hosp_labevents\n",
      "  Partitions: 3\n",
      "\n",
      "Topic: icu_d_items\n",
      "  Partitions: 1\n",
      "\n",
      "Topic: icu_chartevents\n",
      "  Partitions: 1\n",
      "\n",
      "Topic: __consumer_offsets\n",
      "  Partitions: 50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from confluent_kafka.admin import AdminClient\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"SparkProcessor\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .config(\"spark.executor.cores\", \"4\")\n",
    "    # Add Kafka package for reading from Kafka\n",
    "    .config(\"spark.jars.packages\",\n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "KAFKA_BOOTSTRAP_SERVERS = \"cp-kafka:9092\"\n",
    "admin_client = AdminClient({\n",
    "    'bootstrap.servers': KAFKA_BOOTSTRAP_SERVERS\n",
    "})\n",
    "\n",
    "# List all topics\n",
    "try:\n",
    "    metadata = admin_client.list_topics(timeout=10)\n",
    "    print(\"Available Kafka Topics:\")\n",
    "    print(\"-\" * 50)\n",
    "    for topic in metadata.topics.values():\n",
    "        print(f\"Topic: {topic.topic}\")\n",
    "        print(f\"  Partitions: {len(topic.partitions)}\")\n",
    "        print()\n",
    "except Exception as e:\n",
    "    print(f\"Error listing topics: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a99d9f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, LongType\n",
    "\n",
    "chartevents_schema = StructType([\n",
    "    StructField(\"subject_id\", IntegerType()),\n",
    "    StructField(\"hadm_id\", IntegerType()),\n",
    "    StructField(\"stay_id\", IntegerType()),\n",
    "    StructField(\"caregiver_id\", IntegerType()),\n",
    "    StructField(\"charttime\", StringType()),\n",
    "    StructField(\"storetime\", StringType()),\n",
    "    StructField(\"itemid\", IntegerType()),\n",
    "    StructField(\"value\", StringType()),\n",
    "    StructField(\"valuenum\", DoubleType()),\n",
    "    StructField(\"valueuom\", StringType()),\n",
    "    StructField(\"warning\", StringType())\n",
    "])\n",
    "\n",
    "d_items_schema = StructType([\n",
    "    StructField(\"itemid\", IntegerType()),\n",
    "    StructField(\"label\", StringType()),\n",
    "    StructField(\"abbreviation\", StringType()),\n",
    "    StructField(\"linksto\", StringType()),\n",
    "    StructField(\"category\", StringType()),\n",
    "    StructField(\"unitname\", StringType()),\n",
    "    StructField(\"param_type\", StringType()),\n",
    "    StructField(\"lownormalvalue\", DoubleType()),\n",
    "    StructField(\"highnormalvalue\", DoubleType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f18ece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc16027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/04 15:36:05 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-------------------+--------------+-------------------+--------+-------------+--------------+---------------+\n",
      "|itemid|               label|       abbreviation|       linksto|           category|unitname|   param_type|lownormalvalue|highnormalvalue|\n",
      "+------+--------------------+-------------------+--------------+-------------------+--------+-------------+--------------+---------------+\n",
      "|220003|  ICU Admission date| ICU Admission date|datetimeevents|                ADT|    NULL|Date and time|          NULL|           NULL|\n",
      "|220045|          Heart Rate|                 HR|   chartevents|Routine Vital Signs|     bpm|      Numeric|          NULL|           NULL|\n",
      "|220046|Heart rate Alarm ...|    HR Alarm - High|   chartevents|             Alarms|     bpm|      Numeric|          NULL|           NULL|\n",
      "|220047|Heart Rate Alarm ...|     HR Alarm - Low|   chartevents|             Alarms|     bpm|      Numeric|          NULL|           NULL|\n",
      "|220048|        Heart Rhythm|       Heart Rhythm|   chartevents|Routine Vital Signs|    NULL|         Text|          NULL|           NULL|\n",
      "|220050|Arterial Blood Pr...|               ABPs|   chartevents|Routine Vital Signs|    mmHg|      Numeric|          90.0|          140.0|\n",
      "|220051|Arterial Blood Pr...|               ABPd|   chartevents|Routine Vital Signs|    mmHg|      Numeric|          60.0|           90.0|\n",
      "|220052|Arterial Blood Pr...|               ABPm|   chartevents|Routine Vital Signs|    mmHg|      Numeric|          NULL|           NULL|\n",
      "|220056|Arterial Blood Pr...|    ABP Alarm - Low|   chartevents|             Alarms|    mmHg|      Numeric|          NULL|           NULL|\n",
      "|220058|Arterial Blood Pr...|   ABP Alarm - High|   chartevents|             Alarms|    mmHg|      Numeric|          NULL|           NULL|\n",
      "|220059|Pulmonary Artery ...|               PAPs|   chartevents|       Hemodynamics|    mmHg|      Numeric|          15.0|           25.0|\n",
      "|220060|Pulmonary Artery ...|               PAPd|   chartevents|       Hemodynamics|    mmHg|      Numeric|           8.0|           15.0|\n",
      "|220061|Pulmonary Artery ...|               PAPm|   chartevents|       Hemodynamics|    mmHg|      Numeric|          10.0|           20.0|\n",
      "|220063|Pulmonary Artery ...|   PAP Alarm - High|   chartevents|             Alarms|    mmHg|      Numeric|          NULL|           NULL|\n",
      "|220066|Pulmonary Artery ...|    PAP Alarm - Low|   chartevents|             Alarms|    mmHg|      Numeric|          NULL|           NULL|\n",
      "|220069|Left Artrial Pres...|                LAP|   chartevents|       Hemodynamics|    mmHg|      Numeric|           6.0|           12.0|\n",
      "|220072|Central Venous Pr...|   CVP Alarm - High|   chartevents|             Alarms|    mmHg|      Numeric|          NULL|           NULL|\n",
      "|220073|Central Venous Pr...|    CVP Alarm - Low|   chartevents|             Alarms|    mmHg|      Numeric|          NULL|           NULL|\n",
      "|220074|Central Venous Pr...|                CVP|   chartevents|       Hemodynamics|    mmHg|      Numeric|          NULL|           NULL|\n",
      "|220088|Cardiac Output (t...|CO (thermodilution)|   chartevents|       Hemodynamics|   L/min|      Numeric|           4.0|            8.0|\n",
      "+------+--------------------+-------------------+--------------+-------------------+--------+-------------+--------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json, col, expr\n",
    "\n",
    "df_chartevents_raw = (spark.readStream\n",
    "    .format('kafka')\n",
    "    .option('kafka.bootstrap.servers', KAFKA_BOOTSTRAP_SERVERS)\n",
    "    .option('subscribe', 'icu_chartevents')\n",
    "    .option('startingOffsets', 'earliest')\n",
    "    .option(\"maxOffsetsPerTrigger\", \"50\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "df_d_items_raw = (spark.read\n",
    "    .format('kafka')\n",
    "    .option('kafka.bootstrap.servers', KAFKA_BOOTSTRAP_SERVERS)\n",
    "    .option('subscribe', 'icu_d_items')\n",
    "    .option('startingOffsets', 'earliest')\n",
    "    .option('endingOffsets', 'latest')\n",
    "    .load()\n",
    ")\n",
    "\n",
    "df_chartevents = (df_chartevents_raw\n",
    "    .select(from_json(col('value').cast('string'), chartevents_schema).alias('data'))\n",
    "    .select('data.*')\n",
    ")\n",
    "\n",
    "df_d_items = (df_d_items_raw\n",
    "    .select(from_json(col('value').cast('string'), d_items_schema).alias('data'))\n",
    "    .select('data.*')\n",
    ")\n",
    "df_d_items.cache()\n",
    "\n",
    "df_joined = df_chartevents.join(df_d_items, on='itemid', how='left')\n",
    "\n",
    "df_processed = df_joined.withColumn(\"is_critical\", \n",
    "    expr(\"valuenum < lownormalvalue OR valuenum > highnormalvalue\")\n",
    ")\n",
    "\n",
    "sampled_df = sampler.conditional_sample(\n",
    "    df_processed, \n",
    "    condition_col=\"is_critical\", \n",
    "    pass_rate=0.10\n",
    ")\n",
    "\n",
    "query = df_processed.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"path\", './output') \\\n",
    "    .option(\"checkpointLocation\", './checkpoints') \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .trigger(processingTime=\"10 second\") \\\n",
    "    .start()\n",
    "\n",
    "# query = (df_d_items.write\n",
    "#     .format(\"console\")\n",
    "#     .option(\"truncate\", \"false\")\n",
    "#     .trigger(processingTime=\"5 seconds\")\n",
    "#     .start()\n",
    "# )\n",
    "\n",
    "query.awaitTermination(timeout=300)\n",
    "\n",
    "df_d_items.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
